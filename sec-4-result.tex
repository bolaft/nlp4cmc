
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\section{Experiments}
\label{sec:experiments}

% FIXME Table \ref{fig:results} présente / résume / fusionne tous les résultats ; phrase qui annonce les différentes étapes



Table~\ref{fig:results} shows the summary of all obtained results. On the left side are shown results about segmentation metrics, on the right side results about information retrieval metrics. First, we examine baseline scores, and display them in the top section. Second, in the middle section, we show results for segmenters based on individual feature sets (with $A$ standing for $n$-grams, $B$ for information structure, $C$ for TextTiling and $D$ for miscellaneous features). Finally, in the lower section, we show results based on feature sets combinations.


\begin{table}[h]
	\begin{tabular}{p{2.5cm}|C{1.25cm}|C{1.25cm}|C{1.25cm}||C{1.25cm}|C{1.25cm}|C{1.25cm}|}
		\cline{2-7}
		& \multicolumn{3}{c||}{Segmentation metrics} & \multicolumn{3}{c|}{Information Retrieval metrics} \\ \cline{2-7}
		& $WD$ & $P_{k}$ & $GHD$ & $P$ & $R$ & $F_1$ \\ \hline
		\multicolumn{1}{|l|}{regular baseline} 					& .59 & .25 & .60 & .31 & .49 & .38  \\ \hline
		\multicolumn{1}{|l|}{TextTiling baseline} 				& .41 & .07 & .38 & .75 & .44 & .56 \\ \hline\hline
		\multicolumn{1}{|l|}{$\phi(A)$ with $A$ = $n$-grams} 						& .38 & \textbf{.05} & .39 & \textbf{1} & .39 & .56 \\ \hline
		\multicolumn{1}{|l|}{$\phi(B)$ with $B$ = info. structure} 			& .43 & .11 & .38 & .60 & .68 & \textbf{.64} \\ \hline
		\multicolumn{1}{|l|}{$\phi(C)$ with $C$ = TextTiling}				& .39 & .05 & .38 & .94 & .40 & .56 \\ \hline
		\multicolumn{1}{|l|}{$\phi(D)$ with $D$ = misc. features} 					& .41 & .09 & .38 & .69 & .49 & .57 \\ \hline\hline
		\multicolumn{1}{|l|}{$\phi(A + B + C + D)$} 						& .38 & \textbf{.05} & .39 & \textbf{1} & .39 & .56\\ \hline
		\multicolumn{1}{|l|}{$\phi(\phi(A) + \phi(B) + \phi(C) + \phi(D))$} 	& .38 & .06 & .36 & .81 & .47 & .59 \\ \hline
		\multicolumn{1}{|l|}{$\phi(A) \cup \phi(B + C + D)$} 	& .45 & .12 & .40 & .58 & \textbf{.69} & .63 \\ \hline
		\multicolumn{1}{|l|}{$\phi(A) \cup \delta(\phi(B + C + D))$} 	& \textbf{.36} & .06 & \textbf{.34} & .80 & .53 & \textbf{.64} \\ \hline
	\end{tabular}
	\caption{Comparative results between baselines and tested segmenters. All displayed results show \textit{WindowDiff} (\textit{WD}), $P_{k}$ and \textit{GHD} as error rates, therefore a lower score is desirable for these metrics. This contrasts with the three IR scores, for which a low value denotes poor performance. Best scores are shown bolded.}
	\label{fig:results}
\end{table}

\subsection{Baseline segmenters}

The first section of Table \ref{fig:results} shows the results obtained by both of our baselines. Unsurprisingly, TextTiling performs much better than the basic regular segmentation algorithm across all metrics save recall.

\subsection{Segmenters based on individual feature sets}

The second section of Table \ref{fig:results} shows the results for four different classifiers, each trained with a distinct subset of the feature set. The $\phi$ function is the classification function, its parameters are features, and its output a prediction. While all classifiers easily beat the regular baseline, and match the TextTiling baseline when it comes to IR metrics, only the thematic and the $n$-grams segmenters manage to surpass TextTiling when performance is measured by segmentation metrics. In terms of IR scores, the $n$-grams classifier in particular stands out as it manages to achieve an outstanding 100\% precision, although this result is mitigated by a meager 39\% recall. It is also interesting to see that the thematic classifier, based only on contextual information about TextTiling output, performs better than the TextTiling baseline.


\subsection{Segmenters based on feature sets combinations}

The last section of Table \ref{fig:results} shows the results of four different segmenters. The first one, $\phi(A + B + C + D)$, is a simple classifier that takes all available features into account. Its results are exactly identical to that of the $n$-grams classifier, most certainly due to the fact that other features are filtered out due to the sheer number of lexical features. The second one, $\phi(\phi(A) + \phi(B) + \phi(C) + \phi(D))$, uses as features the outputs of the four classifiers trained on each individual feature set. Results show this approach isn't significantly better. The third one, $\phi(A) \cup \phi(B + C + D)$, segments according to the union of the boundaries detected by a classifier trained on $n$-grams features and those identified by a classifier trained on all other features. This idea is motivated by the fact that we know all boundaries found by the $n$-grams classifier to be accurate ($P=1$). Doing this allows the segmenter to obtain the best possible recall ($R=.69)$, but at the expense of precision ($P=.58$). The last one, $\phi(A) \cup \delta(\phi(B + C + D))$, attempts to increase the $n$-grams classifier's recall without sacrificing too much precision by being more selective about boundaries. The $\delta$ function is the "cherry picking" function, which filters out boundaries predicted without sufficient confidence. Only those identified by the $n$-grams classifier and those classified as boundaries with a confidence score of at least .99 by a classifier trained on the other feature sets are considered. This system outperforms all others both in terms of segmentation scores and $F_1$, however it is still relatively conservative and the segmentation ratio (the number of true boundaries divided by the number of guessed boundaries) remains significantly lower than expected, at~0.67. Tuning the minimum confidence score ($c$) allows to adjust $P$ from .58 ($c$ = 0) to 1 ($c$ = 1) and $R$ from .39 ($c$ = 1) to .69 ($c$ = 0).