
% -----------------------------------------------------------------------------
\section{Building the segmenter}
\label{sec:buildingTheSegmenter}
Each email is processed as a sequence of sentences.
We choose to define the segmentation problem as a sequence labelling task whose aim is to assign the globally best set of labels for the entire sequence at once. The underlying idea is that the choice of the optimal label for a given sentence is dependent on the choices of nearby sentences.
% If a sentence is labelled \texttt{\footnotesize ending}, the next one is likely to be a \texttt{\footnotesize starting}.
%
% wikipedia is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values. A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document. Sequence labeling can be treated as a set of independent classification tasks, one per member of the sequence. However, accuracy is generally improved by making the optimal label for a given element dependent on the choices of nearby elements, using special algorithms to choose the globally best set of labels for the entire sequence at once.
%
Our email segmenter is built around a linear-chain Conditional Random Field (CRF), as implemented in the sequence labelling toolkit Wapiti \cite{lavergne2010practical}.

Training the classifier to recognize the different labels of the previously defined annotation scheme can be problematic. It has indeed some disadvantages that can undermine the effectiveness of the classifier. In particular, sentences annotated \textit{SE} will, by definition, share important characteristics with sentences bearing the annotation \textit{S} and \textit{E}. 
%For the purposes of segmentation, 
So we chose to transform these annotations into a binary scheme and merely differentiate sentences that starts a new segment (\textit{True}), or "boundary sentences", from those that do not (\textit{False}). The conversion process is trivial, and can easily be reversed\footnote{Sentences labelled with \textit{SE} or \textit{S} are turned into \textit{True}, the other ones into \textit{False}. To reverse the process, a \textit{True} is turned into \textit{SE} if the next sentence is also a boundary (i.e. a True) and into \textit{S} otherwise. While a \textit{False} is turned into \textit{E} if the next sentence is a boundary (i.e. a True) and into  \textit{I} otherwise.}.

We distinguish four sets of features:  $n$-gram features, information structure based features, thematic features and miscellaneous features.
%The purpose of each one of these features is to describe the current sentence from which it is extracted. 
All the features are domain-independant. Almost all features are language-independant as well, save for a few that can be easily translated.
For our experiments, the CRF window size is set at 5, i.e. the classification algorithm takes into account features of the next and previous two sentences as well as the current one.



% -----------------------------------------------------------------------------
\paragraph{$n$-gram features}
%
We select the case-insensitive word bigrams and trigrams with the highest document frequency in the training data (empirically we select the top 1,000 $n$-grams), and check for their presence in each sentence. Since the probability of having multiple occurrences of the same $n$-gram in one sentence are extremely low, we do not record the number of occurrences but merely a boolean value. %The check is case-insensitive.


% -----------------------------------------------------------------------------
\paragraph{Information structure based features}

This feature set is inspired by the information structure theory \cite{kruijff:1996} which 
describes the information imparted by the sentence in terms of the way it is related to prior context. %: topic, (back)ground, focus, new information\ldots
 The theory relates these functions with particular syntactic constructions (e.g. topicalization) and word order constraints in the sentence.
%
%For languages such as English or French, the beginning of the sentence is an important position to structure the information at the discourse level while the ending of the sentence may carry information to announce what the following will deal with.
%The beginning of a sentence is so often seen as playing an important role in discourse structure 

We focus on the first and last three \textit{significant} tokens in the sentence. %, which often serve as connectors to adjoining sentences and therefore hold important structural information.
A token is considered as significant if its occurrence frequency is higher than  1/2,000\footnote{This value was set up empirically on our data. More experimentation needs to be done to generalize it.}.
%Token significance is determined by the number of occurrences of the token in the training corpus: terms with a frequency lower than 1/2000 are considered insignificant. If a sentence contains less than six significant tokens, the same token can be found in both triplets. If the sentence contains less than three significant tokens, missing values are replaced by a placeholder.
As features we use $n$-grams of the surface form, lemma and part-of-speech tag of each triplet (36 features).
%For example, from the sentence "\textit{Have you tried to update your browser?}", we can generate the following features~: \texttt{\footnotesize "Have", "have", "NNP", "you", "you", "PRP", "tried", "try", "VBD", "Have you", "have you", "you tried", "you try", "NNP PRP", "PRP VBD", "Have you tried", "have you try", "NNP PRP VBD", "your", "your", "PRP"} etc.
%
%We define three individual features for each of the three unigrams, the two bigrams and the single trigram found in each of these triplets. The features are the following: the unaltered form of each token (case-sensitive), their lemmatized form (case-insensitive) and their corresponding part-of-speech. This is illustrated in figure ~\ref{fig:exampleSyntacticFeatures}.
%

%\begin{figure}\small\centering
%\begin{tabular}{*{2}{|l}|l|}
%\toprule
%\textbf{Surface form} & \textbf{Lemma} & \textbf{P.O.S.}\\
%	\midrule
%	Many & many & JJ\\
%	thanks & thanks & NNS\\
%	to & to & TO\\
%	your & your & PRP\\
%	suggestions & suggestion & DD\\
%	. & . & .\\
%	Many thanks & many thanks & JJ NNS\\
%	thanks to . & thanks to . & NNS TO . \\
%	your suggestions & your suggestion & PRP DD\\
%	suggestions & suggestion & DD\\
%	Many thanks to & many thanks to & JJ NNS TO\\
%	your suggestions . & your suggestion . & PRP DD .\\
%	\bottomrule
%\end{tabular}

%\caption{Syntactic features formed from the sentence "\textit{Many thanks to all of you for the help you have offered, I have learned tremendously from all your suggestions.}" Each cell is a feature.}
%\label{fig:exampleSyntacticFeatures}
%\end{figure}

% -----------------------------------------------------------------------------
\paragraph{Thematic feature}
%
The only feature we use to account for thematic shift recognition is the output of the TextTiling algorithm \cite{hearst1997texttiling}. TextTiling is one of the most commonly used algorithms for automatic text segmentation. 
%It is a statistical method for thematic segmentation, i.e. a method that attempts to segment texts according to the contrasting themes of its parts: 
If the algorithm detects a rupture in the lexical cohesion %thematic continuity 
of the text (between two consecutive blocks), it will place a boundary to indicate a thematic change. %This method is based on the notion of lexical cohesion.
%
Due to the short size of the messages, we define a block size to equate the sum of three times the sentence average size in our corpus. We set the step-size (overlap size of the rolling window) to the average size of a sentence.

% -----------------------------------------------------------------------------
\paragraph{Miscellaneous features}
%
This feature set includes stylistic %graphic, orthographic 
and semantic features. 24 features, several of them borrowed from related work in speech act classification \cite{qadir2011classifying} and email segmentation \cite{lampert2009segmenting}, are in the set:
%
\textit{Stylistic features} capture information about the visual structure and composition of the message:
the position of the sentence in the email, 
the average length of a token,
the total number of tokens and characters, 
the proportion of upper-case, alphabetic and numeric characters,
%\textit{Orthographic features} capture information about the use of distinctive characters or character sequences:
the number of greater-than signs (``>''); %also know as ``chevron'' symbols, in the sentence
whether the sentence ends with or contains a question mark, a colon or a semicolon; %, or if it at least contains one (three features)
whether the sentence contains any punctuation within the first three tokens (this is meant to recognize greetings \cite{qadir2011classifying}).
%\end{itemize}

\textit{Semantic features} check for meaningful words and phrases:
%
%\begin{itemize}
%	\item 
whether the sentence begins with or contains a ``wh*'' question word 
% (\textit{``who''}, \textit{``when''}, \textit{``where''}, \textit{``what''}, \textit{``which''}, \textit{``what''}, \textit{``how''}) 
or a phrase %an $n$-gram 
suggesting an incoming interrogation (e.g. \textit{``is it''}, \textit{``are there''}); %, and whether the sentence merely contains such a word or $n$-gram
%	\item 
whether the sentence contains a modal; 
%(\textit{``can''}, \textit{``may''}, \textit{``must''}, \textit{``shall''}, \textit{``will''}, \textit{``might''}, \textit{``should''}, \textit{``would''}, \textit{``could''}, and their negative forms);
%	\item 
whether any plan phrases (e.g. \textit{``i will''}, \textit{``we are going to''}) are present;
%	\item 
whether the sentence contains first person (e.g. \textit{``we''}, \textit{``my''}) % and \textit{``me''}), 
second person or third person words; % (e.g. \textit{``we''}, \textit{``my''} and \textit{``me''} are recognized as first person words)
%	\item 
the first personal pronoun found in the sentence;
%	\item 
the first verbal form found.% in the sentence as classified by the Stanford Part-Of-Speech tagger ; that is an element of the Penn Treebank tag set \footnote{Alphabetical list of part-of-speech tags used in the Penn Treebank Project: \url{http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html}} (e.g. the feature \textit{``VBZ''} indicates a present tense verb in third person singular)

%\begin{itemize}
%    \item \textit{SE} $\longrightarrow$ \textit{True} $\longrightarrow$ \textit{SE} (if the next sentence is also a boundary)
%    \item \textit{S} $\longrightarrow$ \textit{True} $\longrightarrow$ \textit{S} (if the next sentence is not a boundary)
%    \item \textit{I} $\longrightarrow$ \textit{False} $\longrightarrow$ \textit{I} (if the next sentence is not a boundary either)
%    \item \textit{E} $\longrightarrow$ \textit{False} $\longrightarrow$ \textit{E} (if the next sentence is a boundary)
%\end{itemize}
